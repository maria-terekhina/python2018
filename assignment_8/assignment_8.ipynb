{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8\n",
    "\n",
    "Develop a model for 20 news groups dataset. Select 20% of data for test set.  \n",
    "\n",
    "Use metric learning with siamese networks and triplet loss.   \n",
    "Use KNN and LSH (`annoy` library) for final prediction after the network was trained.\n",
    "\n",
    "! Remember, that LSH gives you a set of neighbor candidates, for which you have to calculate distances to choose top-k nearest neighbors. \n",
    "\n",
    "Your quality = accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "spacy_en.remove_pipe('tagger')\n",
    "spacy_en.remove_pipe('ner')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.lemma_ for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.target\n",
    "train = train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, valid, y_test, y_valid = test.data[:len(test.data)//2], test.data[len(test.data)//2:], test.target[:len(test.data)//2], test.target[len(test.data)//2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Переводит тектсы с списки индексов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_to_idx из Elmo у меня не работает, все падает. Написала сама такую же функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(data):\n",
    "    word2id = dict()\n",
    "    id2word = dict()\n",
    "    i = 1\n",
    "    max_len = 0\n",
    "    \n",
    "    for dtype in data:\n",
    "        for comment in dtype:\n",
    "            tokens = tokenizer(comment)\n",
    "            if len(tokens) > max_len:\n",
    "                max_len = len(tokens)\n",
    "            \n",
    "            for t in tokens:\n",
    "                if not t in word2id:\n",
    "                    word2id[t] = i\n",
    "                    id2word[i] = t\n",
    "                    i += 1\n",
    "                    \n",
    "    return word2id, id2word, max_len\n",
    "\n",
    "def batch_to_ids(dtype, word2id, max_len):\n",
    "    list_of_ids = list()\n",
    "    \n",
    "    for comment in tqdm_notebook(dtype):\n",
    "        comment_ids = list()\n",
    "        tokens = tokenizer(comment)\n",
    "        \n",
    "        for t in tokens:\n",
    "            comment_ids.append(word2id[t])\n",
    "        while len(comment_ids) < max_len:\n",
    "            comment_ids.append(0)\n",
    "        list_of_ids.append(comment_ids)\n",
    "        \n",
    "    return list_of_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id, id2word, max_len = get_ids((train, valid, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a07c10f2fa84909b912f3b27235f8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3766), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeb57e8c3c04f849c8bc6d21f02bbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdad3b65d4b4643b54adc38da4f4f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3766), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_id = batch_to_ids(test, word2id, max_len)\n",
    "train_id = batch_to_ids(train, word2id, max_len)\n",
    "valid_id = batch_to_ids(valid, word2id, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Делаем тройки даных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triplets(data, y):\n",
    "    positive = list()\n",
    "    negative = list()\n",
    "    for i, comment in enumerate(data):\n",
    "        positive.append(data[np.random.choice(np.where(y == y[i])[0])])\n",
    "        negative.append(data[np.random.choice(np.where(y != y[i])[0])])\n",
    "    \n",
    "    return tt.tensor(positive), tt.tensor(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "positive, negative = make_triplets(test_id, y_test)\n",
    "test_loader = DataLoader(TensorDataset(tt.tensor(test_id), positive, negative), batch_size=batch_size, drop_last=True)\n",
    "\n",
    "positive, negative = make_triplets(train_id, y_train)\n",
    "train_loader = DataLoader(TensorDataset(tt.tensor(train_id), positive, negative), batch_size=batch_size, drop_last=True)\n",
    "\n",
    "positive, negative = make_triplets(valid_id, y_valid)\n",
    "valid_loader = DataLoader(TensorDataset(tt.tensor(valid_id), positive, negative), batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model, iterator, optimizer, curr_epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    iterator = tqdm_notebook(iterator, total=n_batches, desc='epoch %d' % (curr_epoch), leave=True)\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(batch[0], batch[1], batch[2])\n",
    "        loss = tt.abs(tt.mean(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        curr_loss = loss.data.cpu().detach().item()\n",
    "        \n",
    "        loss_smoothing = i / (i+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "        iterator.set_postfix(loss='%.5f' % running_loss)\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "def _test_epoch(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    n_batches = len(iterator)\n",
    "    with tt.no_grad():\n",
    "        for batch in iterator:\n",
    "            loss = model(batch[0], batch[1], batch[2])\n",
    "            loss = tt.abs(tt.mean(loss))\n",
    "            epoch_loss += loss.data.item()\n",
    "\n",
    "    return epoch_loss / n_batches\n",
    "\n",
    "\n",
    "def nn_train(model, train_iterator, valid_iterator, optimizer, n_epochs=100,\n",
    "          scheduler=None, early_stopping=0):\n",
    "\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    best_epoch = None\n",
    "    history = pd.DataFrame()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = _train_epoch(model, train_iterator, optimizer, epoch)\n",
    "        valid_loss = _test_epoch(model, valid_iterator)\n",
    "\n",
    "        #valid_loss = valid_loss\n",
    "        print('validation loss %.5f' % valid_loss)\n",
    "\n",
    "        record = {'epoch': epoch, 'train_loss': train_loss, 'valid_loss': valid_loss}\n",
    "        history = history.append(record, ignore_index=True)\n",
    "\n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "\n",
    "            if es_epochs >= early_stopping:\n",
    "                best_epoch = history[history.valid_loss == history.valid_loss.min()].iloc[0]\n",
    "                print('Early stopping! best epoch: %d val %.5f' % (best_epoch['epoch'], best_epoch['valid_loss']))\n",
    "                break\n",
    "\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor_embed, pos_embed, neg_embed):\n",
    "    return F.cosine_similarity(anchor_embed, neg_embed) - F.cosine_similarity(anchor_embed, pos_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tripletnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, criterion):\n",
    "        super(Tripletnet, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        self.fc = nn.Linear(embed_size, 128)\n",
    "        \n",
    "        self.out = nn.Linear(128*3, 20)\n",
    "        \n",
    "    def branch(self, x):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, anchor, pos, neg):\n",
    "        \n",
    "        anchor = self.branch(anchor)\n",
    "        pos = self.branch(pos)\n",
    "        neg = self.branch(neg)\n",
    "        \n",
    "        return triplet_loss(anchor, pos, neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b336db4af592452d9bbfd22385e77a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 0', max=353), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeca5aedfd840b8a55a08aca220dbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=353), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.00000\n"
     ]
    }
   ],
   "source": [
    "model = Tripletnet(len(word2id), 100, nn.BCEWithLogitsLoss())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "nn_train(model, train_loader, valid_loader, optimizer, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Masha\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Tripletnet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "tt.save(model, 'model_ass8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tt.load('model_ass8.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52ac43ea2004c58beb29c9292c91d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 128\n",
    "a = AnnoyIndex(f)\n",
    "train_emb = list()\n",
    "\n",
    "for i in tqdm_notebook(range(1, len(train_id))):    \n",
    "    emb = model.branch(tt.tensor(train_id[i-1:i])).detach().numpy()\n",
    "    train_emb.append(emb[0])\n",
    "    a.add_item(i, emb[0])\n",
    "a.build(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30809696d20f4142bc437e618dcdd3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3755), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_emb = list()\n",
    "\n",
    "for i in tqdm_notebook(range(1, len(test_id))):    \n",
    "    emb = model.branch(tt.tensor(test_id[i-1:i])).detach().numpy()\n",
    "    test_emb.append(emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"test_emb.npy\", np.array(test_emb))\n",
    "np.save(\"train_emb.npy\", np.array(train_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb696dd43a14033a0524865a56f3a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.03089214380825566\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = 0\n",
    "\n",
    "for i, v in tqdm_notebook(enumerate(test_emb)):\n",
    "\n",
    "    ten_nearest = a.get_nns_by_vector(v, 10)\n",
    "    nearest_v = list()\n",
    "    nearest_y = list()\n",
    "\n",
    "    for n in ten_nearest:\n",
    "        n_vec = train_emb[n]\n",
    "        real_train = y_train[n]\n",
    "        nearest_v.append(list(n_vec))\n",
    "        nearest_y.append(real_train)\n",
    "\n",
    "    pred = nearest_v[cKDTree(nearest_v).query(v, k=1)[1]]\n",
    "    id_pred = nearest_v.index(pred)\n",
    "    if y_test[i] == nearest_y[id_pred]:\n",
    "        accuracy_score += 1\n",
    "        \n",
    "print('Accuracy:', accuracy_score/len(test_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbba2d0319144695af21720bcf12a6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.05193075898801598\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = 0\n",
    "for i, v in tqdm_notebook(enumerate(test_emb)):\n",
    "\n",
    "    ten_nearest = a.get_nns_by_vector(v, 10)\n",
    "    nearest_v = list()\n",
    "    nearest_y = list()\n",
    "\n",
    "    for n in ten_nearest:\n",
    "        n_vec = train_emb[n]\n",
    "        real_train = y_train[n]\n",
    "        nearest_v.append(n_vec)\n",
    "        nearest_y.append(real_train)\n",
    "    \n",
    "       \n",
    "    if y_test[i] == max(set(nearest_y), key = nearest_y.count):\n",
    "        accuracy_score += 1\n",
    "        \n",
    "print('Accuracy:', accuracy_score/len(test_emb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
